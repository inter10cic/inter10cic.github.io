---
title: "Study 2: Descending into ML"
date: 2021-01-08 20:48:13 -0400
categories: deeplearning machinelearning
---

# Descending into ML

## Linear Regression

A linear relationship can be drawn when the independent and dependent variables expressed on a grid can be drawn with a straight line. Mathematically, this can be expressed as 
y = mx +b, where:

-	y = value of the label (the value we are trying to predict)
-	m = slope of line
-	x = value of the input feature
-	b = y-intercept

In machine learning, this equation is shaped a little differently in the model. 
It is y’ = b + w<sub>1</sub>x<sub>1</sub>, where:

-	y’ = predicted label (desired output)
-	b = bias (y-intercept)
-	w<sub>1</sub> = weight of feature 1
-	x<sub>1</sub> = value of the input feature (already known)

![LR graph](https://developers.google.com/machine-learning/crash-course/images/CricketPoints.svg "Linear regression model")
Above is an example of a linear regression model.

Inference through the model is done by inserting the value of the features (x<sub>1</sub>s) into the above equation. A more sophisticated model looks like: y’ = b + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + w<sub>3</sub>x<sub>3</sub>.

## Training and Loss

__Training__, as aforementioned, is the process of teaching the model the relationship between features and labels through good values. 

__Supervised learning__, however, is the process of building a model that minimizes loss – __empirical risk minimization__.

__Loss__: a number indicating how bad the model’s prediction was on a single example

We don’t need more losses, so the ultimate goal of training a model is finding weights and biases that have low loss, so empirical risk minimization can occur. 

![Loss comparison](https://developers.google.com/machine-learning/crash-course/images/LossSideBySide.png "Loss comparison")
The graphs above show a model with a high loss and a model with a low loss

The most popular loss function is squared loss, or L2 loss. It is basically the square of the difference between the label and the prediction. Mathematically, the equation is (observation – prediction(x))<sup>2</sup>.

__Mean square error(MSE)__ is the average value of the squared loss over the whole data set. 

Equation wise it shows this:

$$MSE = {1\over N} {\sum\limits_{(x,y)\in D}}{(y-prediction(x))}^{2}$$

Here, prediction(x) is the function that takes into account the weights and biases using the features x.
